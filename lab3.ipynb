{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Unsupervised Learning (Advanced Clustering)\n",
    "\n",
    "For this lab we will use Scikit-Learn’s API, sklearn.datasets, which allows us to access a famous dataset for linguistic analysis, the 20newsgroups dataset. A newsgroup is an online user discussion group, such as a forum. Sklearn allows us to access different categories of content. We will use texts that have to do with technology, religion and sport.\n",
    "\n",
    "More details about the dataset: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html \n",
    "\n",
    "Your task is to perform clustering on the given dataset. \n",
    "\n",
    "**Submission: submit via onq.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries. E.g., pandas, sklearn, nltk, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Read the dataset and report the basic statistics of the dataset\n",
    "import data from sklearn.datasets and selecte pre-defined categories:\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'alt.atheism',\n",
    "]\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code for task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Data cleaning.\n",
    "Use standard text preprocessing steps to preprocess raw textual content. \n",
    "A sample preprocessing function is provided as below.\n",
    "\n",
    "def preprocess_text(text: str, remove_stopwords: bool) -> str:\n",
    "    \"\"\"This utility function sanitizes a string by:\n",
    "    - removing links\n",
    "    - removing special characters\n",
    "    - removing numbers\n",
    "    - removing stopwords\n",
    "    - transforming in lowercase\n",
    "    - removing excessive whitespaces\n",
    "    Args:\n",
    "        text (str): the input text you want to clean\n",
    "        remove_stopwords (bool): whether or not to remove stopwords\n",
    "    Returns:\n",
    "        str: the cleaned text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove links\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # remove special chars and numbers\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # remove stopwords\n",
    "    if remove_stopwords:\n",
    "        # 1. tokenize\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 2. check if stopword\n",
    "        tokens = [w for w in tokens if not w.lower() in stopwords.words(\"english\")]\n",
    "        # 3. join back together\n",
    "        text = \" \".join(tokens)\n",
    "    # return text in lower case and stripped of whitespaces\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code for task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: create vector representation of each document using TF-IDF encoding, if we don't know what is TF-IDF encoding, read this: https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code for task 3, hint, you can use TfidfVectorizer from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Apply Kmeans and Kmeans++ on the above extracted document vectors. Brifely describe how would you select the value of k. \n",
    "You can use https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html#sklearn.cluster.k_means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code for Task 4. Note, you just need to change init parameter to swtich between random intilization and k-means++ initialization. \n",
    "#init{‘k-means++’, ‘random’}, callable or array-like of shape (n_clusters, n_features), default=’k-means++’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: perform tricks on the above two approaches by considering the high-dimensional nature of the dataset, you can consider PCA (from sklearn.decomposition import PCA), or other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: compare the performance of the above four approaches on the given dataset using mutual information based scores: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 6"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
